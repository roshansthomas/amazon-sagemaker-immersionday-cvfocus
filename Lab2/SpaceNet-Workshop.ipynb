{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaceNet on AWS\n",
    "\n",
    "\n",
    "SpaceNet is a corpus of commercial satellite imagery and labeled training data to use for machine learning research. The dataset is currently hosted as an Amazon Web Services (AWS) Public Dataset. This notebook will apply a custom MXNet-based U-Net algorithm to build a neural network that will automatically detect buildings in the dataset using machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute when first running the notebook\n",
    "# !conda install -c conda-forge shapely -y\n",
    "\n",
    "!pip install shapely\n",
    "!pip install tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"opencv-python-headless<4.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://wwps-sagemaker-workshop/spacenet/utils . --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet, MXNetModel\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from shapely.wkt import loads as wkt_loads\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shapely.geometry\n",
    "import imageio\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "in_bucket = 'wwps-sagemaker-workshop'\n",
    "prefix = 'spacenet/dataset/'\n",
    "\n",
    "def load_training_image(bucket,image_key):\n",
    "    key = prefix + image_key\n",
    "    return tiff.imread(BytesIO(s3.Object(bucket,key).get()['Body'].read()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load RGB bands and transpose image shape\n",
    "\n",
    "test = load_training_image(in_bucket,'AOI_3_Paris_Train/RGB-PanSharpen/RGB-PanSharpen_AOI_3_Paris_img100.tif')\n",
    "plt.imshow(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image looks weird because its actually an 11-bit image stored in 16 bit integers. You can try to figure out what that means, or you can just run the next cell which will make everything look better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_bands(img, lower_pct = 1, upper_pct = 99):\n",
    "    \"\"\"\n",
    "    Rescale the bands of a multichannel image for display\n",
    "    \"\"\"\n",
    "    # Loop through the image bands, rescaling each one\n",
    "    img_scaled = np.zeros(img.shape, np.uint8)\n",
    "    \n",
    "    for i in range(img.shape[2]):\n",
    "        \n",
    "        band = img[:, :, i]\n",
    "        \n",
    "        # Pick out the lower and upper percentiles\n",
    "        lower, upper = np.percentile(band, [lower_pct, upper_pct])\n",
    "        \n",
    "        # Normalize the band\n",
    "        band = (band - lower) / (upper - lower) * 255\n",
    "        \n",
    "        # Clip the high and low values, and cast to uint8\n",
    "        img_scaled[:, :, i] = np.clip(band, 0, 255).astype(np.uint8)\n",
    "        \n",
    "    return img_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rescaled image\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.imshow(scale_bands(test))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Masks for our Training Dataset\n",
    "\n",
    "We will use the SpaceNet Utilities to create masks from the geojson data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_polygon_list(wkt_list_pandas, imageId):\n",
    "    df_image = wkt_list_pandas[wkt_list_pandas.ImageId == imageId]\n",
    "    poly_def = df_image.PolygonWKT_Pix\n",
    "    polygonList = None\n",
    "    polygonList = [wkt_loads(x) for x in poly_def]\n",
    "    return polygonList, poly_def\n",
    "\n",
    "\n",
    "def _get_and_convert_contours(polygonList, raster_img_size, poly_def):\n",
    "    perim_list = []\n",
    "    interior_list = []\n",
    "    if len(poly_def) < 2:\n",
    "        return None\n",
    "    for k in range(len(poly_def)):\n",
    "        poly = polygonList[k]\n",
    "        perim = np.array(list(poly.exterior.coords))\n",
    "        perim_c = np.array(perim[:,:-1]).astype(int)\n",
    "        perim_list.append(perim_c)\n",
    "        for pi in poly.interiors:\n",
    "            interior = np.array(list(pi.coords))\n",
    "#            interior_c = _convert_coordinates_to_raster(interior, raster_img_size)\n",
    "            interior_list.append(np.int32(interior[:,:-1]))\n",
    "    return perim_list,interior_list\n",
    "\n",
    "\n",
    "def _plot_mask_from_contours(raster_img_size, ext_pts, int_pts, class_value = 1):\n",
    "    img_mask = np.zeros(raster_img_size,np.uint8)\n",
    "    cv2.fillPoly(img_mask, np.asarray(ext_pts),1)\n",
    "    cv2.fillPoly(img_mask, np.asarray(int_pts),0)\n",
    "    return img_mask\n",
    "\n",
    "def generate_mask_for_image_and_class(raster_size, imageId, wkt_list_pandas):\n",
    "    polygon_list, poly_def = _get_polygon_list(wkt_list_pandas,imageId)\n",
    "    if len(polygon_list) < 2:\n",
    "        mask = np.zeros(raster_size,np.uint8)\n",
    "        return mask\n",
    "    else:\n",
    "        ext, inte = _get_and_convert_contours(polygon_list,raster_size, poly_def)\n",
    "        mask = _plot_mask_from_contours(raster_size,ext, inte,1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BytesIO(s3.Object(in_bucket, prefix+'AOI_3_Paris_Train/summaryData/AOI_3_Paris_Train_Building_Solutions.csv').get()['Body'].read()))\n",
    "\n",
    "def get_image_id(image_file_name):\n",
    "    return image_file_name[48:-4]\n",
    "\n",
    "def list_images_in_bucket():\n",
    "    images = []\n",
    "    s3client = boto3.client('s3')\n",
    "    s3_list_objects_resp = s3client.list_objects_v2(Bucket=in_bucket,Prefix=prefix+'AOI_3_Paris_Train/RGB-PanSharpen')\n",
    "    for obj in s3_list_objects_resp['Contents']:\n",
    "        key = obj['Key'][17:]\n",
    "        if key.lower().endswith('.tif'):\n",
    "            images.append(key)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example masks generated from ground truth polygons\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121)\n",
    "example_mask = generate_mask_for_image_and_class((650,650),'AOI_3_Paris_img100',df)\n",
    "plt.imshow(example_mask)\n",
    "plt.subplot(122)\n",
    "example_mask = generate_mask_for_image_and_class((650,650),'AOI_3_Paris_img211',df)\n",
    "plt.imshow(example_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_nonzero(msk):\n",
    "    \"\"\"Returns the ratio of non-zero values in an array compared to the overall array size\"\"\"\n",
    "    return 1.0 * np.count_nonzero(msk) / msk.size\n",
    "\n",
    "def apply_random_transform(img,msk):\n",
    "    \"\"\"Randomly flip, rotate, or transpose the input arrays.\"\"\"\n",
    "    \n",
    "    # Pick a random transformation, if the input doesn't have a square\n",
    "    # shape limit the options to transformations that do not alter the\n",
    "    # width, height\n",
    "    selected_transform = np.random.randint(0,8)\n",
    "    if img.shape[0] != img.shape[1]:\n",
    "        selected_transform = np.random.randint(0,3)\n",
    "        \n",
    "    if selected_transform == 1:\n",
    "        # Vertical Flip\n",
    "        img = img[::-1,:,:]\n",
    "        msk = msk[::-1,:,:]\n",
    "    elif selected_transform == 2:\n",
    "        # Horizontal Flip\n",
    "        img = img[:,::-1,:]\n",
    "        msk = msk[:,::-1,:]\n",
    "    elif selected_transform == 3:\n",
    "        # Rotate 180\n",
    "        img = np.rot90(img, 2)\n",
    "        msk = np.rot90(msk, 2)\n",
    "    elif selected_transform == 4:\n",
    "        # Transpose\n",
    "        img = img.transpose([1,0,2])\n",
    "        msk = msk.transpose([1,0,2])\n",
    "    elif selected_transform == 5:\n",
    "        # Rotate 90\n",
    "        img = np.rot90(img, 1)\n",
    "        msk = np.rot90(msk, 1)\n",
    "    elif selected_transform == 6:\n",
    "        # Rotate 270\n",
    "        img = np.rot90(img, 3)\n",
    "        msk = np.rot90(msk, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return img,msk\n",
    "\n",
    "\n",
    "def create_image_crops(tiff_images, ground_truth, min_masked=0.1, tile_size=256,stride=100,num_variations=1):\n",
    "    \"\"\"Create pairs of images and masks by sampling regions from larger images and generated ground truth masks.\n",
    "    \n",
    "    This function creates paired lists of images and masks by sampling tiles from the input images. The number\n",
    "    of image mask pairs created per image depends on the ground truth data. A tile will be discarded if the ratio\n",
    "    of masked pixels to the overall tile size is less than the minimum threshold provided (this eliminates training\n",
    "    examples that do not have significant regions to learn). This elimination will reduce the number of \n",
    "    examples in a training set you can choose to create several variations of each tile by applying random flips and\n",
    "    rotations. The number of examples per image can also be increased by decreasing the sampling stride. Doing so \n",
    "    will result in examples with more overlap which will be mitigated to some degree by the application of \n",
    "    random transforms.\n",
    "    \n",
    "    Note that this version of the function loads the images directly from S3 and then generates the masks for \n",
    "    these images on the fly (see generate_mask_for_image_and_class(...) )\n",
    "    Keyword arguments:\n",
    "    min_masked     -- the fraction of an image that must have a non-zero mask to be included (default 0.1)\n",
    "    tile_size      -- the size of the region to sample (default 256)\n",
    "    stride         -- the distance between sampled regions (default 100)\n",
    "    num_variations -- the number of randomly transformed variants for each region (default 1) \n",
    "    \n",
    "    \"\"\"\n",
    "    example_images = []\n",
    "    truth_masks = []\n",
    "    for image_file_name in tiff_images:\n",
    "\n",
    "        \n",
    "        #print('Processing: ' + image_file_name)\n",
    "        \n",
    "        # Load the image from S3 and ensure it exists\n",
    "        image_full = load_training_image(in_bucket, image_file_name)\n",
    "        \n",
    "        # Check to ensure that we can get at least one tile from this image\n",
    "        height = image_full.shape[0]\n",
    "        width = image_full.shape[1]\n",
    "        if height < tile_size or width < tile_size:\n",
    "            continue\n",
    "        \n",
    "        # Generate a mask for this image from the ground truth data\n",
    "        mask_full = generate_mask_for_image_and_class((height,width), get_image_id(image_file_name), ground_truth)\n",
    "            \n",
    "        for row_offset in range(0, height-tile_size, stride):\n",
    "            for column_offset in range(0, width-tile_size, stride): \n",
    "                \n",
    "                row_start = row_offset\n",
    "                row_end = row_start + tile_size\n",
    "                column_start = column_offset\n",
    "                column_end = column_start + tile_size\n",
    "                \n",
    "                mask_tile = mask_full[row_start:row_end, column_start:column_end]\n",
    "                \n",
    "                \n",
    "                if fraction_nonzero(mask_tile) >= min_masked:\n",
    "                    mask_tile = np.expand_dims(mask_tile, axis=0)\n",
    "                    image_tile = image_full[row_start:row_end, column_start:column_end, :]\n",
    "                    image_tile = np.transpose(image_tile, (2, 0, 1))\n",
    "                    for i in range(0,num_variations):\n",
    "                        image_tile,mask_tile = apply_random_transform(image_tile,mask_tile)\n",
    "                        example_images.append(image_tile)\n",
    "                        truth_masks.append(mask_tile)\n",
    "                    \n",
    "\n",
    "    return example_images, truth_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note we are only training on the first 200 images! This is just to save time for the workshop\n",
    "tiff_images = list_images_in_bucket()\n",
    "img, mask = create_image_crops(tiff_images[:200],df,min_masked=0.2,num_variations=2)\n",
    "print('There are ' + str(len(img)) + ' examples in our dataset.')\n",
    "if len(img) != len(mask):\n",
    "    print('Something went horribly wrong!!! We have a different number of lables than we have examples.')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset\n",
    "\n",
    "Now that we know that the code is running - lets train over the entire dataset. Right now the results look questionable but we only had two training images! However, it is generally much easier to experiement on the smaller dataset and confirm that your transformations and code are running properly.\n",
    "\n",
    "To begin, we are going to read in an object from S3 - generate a mask, and then store it in a new folder called \"masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./arrays/'):\n",
    "            os.makedirs('./arrays/')\n",
    "\n",
    "split_index = int(len(img)*0.7)\n",
    "sess_bucket = sagemaker_session.default_bucket()\n",
    "img = np.array(img)\n",
    "mask = np.array(mask)\n",
    "\n",
    "np.save('arrays/train_X_crops.npy', img[:split_index, :, :, :])\n",
    "np.save('arrays/train_Y_crops.npy', mask[:split_index, :, :, :])\n",
    "np.save('arrays/validation_X_crops.npy', img[split_index:, :, :, :])\n",
    "np.save('arrays/validation_Y_crops.npy', mask[split_index:, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='arrays/', key_prefix='full-seg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Job\n",
    "\n",
    "We will run custom code on a p2 instance type. Right now our model will be sub-optimal but thats okay! We will tune it in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_net = MXNet(\"color_segmentation.py\",\n",
    "                  source_dir=\"mxscripts\",\n",
    "                  role=role, \n",
    "                  instance_count=1, \n",
    "#                   instance_type=\"ml.g4dn.4xlarge\",\n",
    "                  instance_type=\"ml.m5.4xlarge\",\n",
    "                  framework_version='1.4.0',\n",
    "                  py_version='py3',\n",
    "                  sagemaker_session=sagemaker_session,\n",
    "                  hyperparameters={\n",
    "#                                  'data_shape': (3, 256, 256),\n",
    "                                 'batch_size': 8,\n",
    "                                 'epochs': 5,\n",
    "                                 'learning_rate': 1E-3\n",
    "#                                  'num_gpus': 1,\n",
    "                                  })\n",
    "\n",
    "sagemaker_net.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_predictor = sagemaker_net.deploy(initial_instance_count=1, instance_type='ml.m4.4xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "test_iter = mx.io.NDArrayIter(data = img[92:93, :, :, :], label=mask[92:93, :, :, :], batch_size=1, shuffle=False)\n",
    "batch = next(test_iter)\n",
    "\n",
    "data = batch.data[0]\n",
    "label = batch.label[0]\n",
    "response = sagemaker_predictor.predict(data.asnumpy().tolist())\n",
    "# response = sagemaker_predictor.predict(data.asnumpy())\n",
    "output = np.array(response[0])\n",
    "print(output.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_mask(label, p=0.5):\n",
    "    return (np.where(label > p, 1, 0)).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 15\n",
    "height = 15\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.subplot(331)\n",
    "plt.title('Input')\n",
    "plt.imshow(scale_bands(np.transpose(data.asnumpy()[0], (1,2,0))))\n",
    "plt.subplot(332)\n",
    "plt.title('Prediction')\n",
    "plt.imshow(post_process_mask(output[0], p=0.9), cmap=plt.cm.gray)\n",
    "plt.subplot(333)\n",
    "plt.title('Actual')\n",
    "plt.imshow(label[0][0].asnumpy(), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 15\n",
    "height = 15\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.subplot(331)\n",
    "plt.title('Input')\n",
    "plt.imshow(scale_bands(np.transpose(data.asnumpy()[0], (1,2,0))))\n",
    "plt.subplot(332)\n",
    "plt.title('Prediction')\n",
    "plt.imshow(post_process_mask(output[0], p=0.9), cmap=plt.cm.gray)\n",
    "plt.subplot(333)\n",
    "plt.title('Actual')\n",
    "plt.imshow(label[0][0].asnumpy(), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaster Response Testing\n",
    "Testing on Hurricane Maria Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_crops = np.load('maria_after_chips.npy')\n",
    "print(test_crops.shape)\n",
    "test_iter = mx.io.NDArrayIter(data = test_crops[351:352, :, :, :], batch_size=1, shuffle=False)\n",
    "batch = next(test_iter)\n",
    "\n",
    "data = batch.data[0]\n",
    "print(data.shape)\n",
    "#response = sagemaker_predictor.predict(data.asnumpy().tolist())\n",
    "response = sagemaker_predictor.predict(data.asnumpy())\n",
    "output = np.array(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example masks generated from ground truth polygons\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(scale_bands(np.transpose(data.asnumpy()[0], (1,2,0))))\n",
    "plt.subplot(122)\n",
    "plt.imshow(post_process_mask(output[0], p=0.1), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "def smooth(img, filter_type):\n",
    "    if filter_type == \"mean\":\n",
    "        return cv.blur(img, (5,5))\n",
    "    if filter_type == \"gaussian\":\n",
    "        return cv.GaussianBlur(img, (5,5), 0)\n",
    "    if filter_type == \"median\":\n",
    "        return cv.medianBlur(img, 5)\n",
    "    else:\n",
    "        return cv.bilateralFilter(img, 9, 75, 75)\n",
    "    \n",
    "def contrast(img, alpha=1.3, beta=40):\n",
    "    return cv.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "\n",
    "def gamma(img, gamma=0.8):\n",
    "    lookUpTable = np.empty((1,256), np.uint8)\n",
    "    for i in range(256):\n",
    "        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
    "    \n",
    "    res = cv.LUT(img, lookUpTable)\n",
    "    return res\n",
    "\n",
    "def detect_veg(img):\n",
    "    # convert to grayscale\n",
    "    gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # threshold to convert to binary image\n",
    "    ret, thresh = cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
    "\n",
    "    # erode image to isolate the sure foreground\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    opening = cv.morphologyEx(thresh,cv.MORPH_OPEN, kernel, iterations=3)\n",
    "\n",
    "    # get the median pixel value (should be background)\n",
    "    mode = img.mean(axis=0).mean(axis=0)\n",
    "\n",
    "    # replace the foreground (trees) with the median pixel\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            # if it's white in the eroded image, then it's vegetation\n",
    "            if opening[i,j] == 255:\n",
    "                # set to black\n",
    "                img[i,j] = mode\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = scale_bands(np.transpose(data.asnumpy()[0], (1,2,0)))\n",
    "smoothed = smooth(new, 'bilateral filter')\n",
    "con = contrast(smoothed, 1.3, 40)\n",
    "gam = detect_veg(con)\n",
    "\n",
    "# Display example masks generated from ground truth polygons\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(scale_bands(np.transpose(data.asnumpy()[0], (1,2,0))))\n",
    "plt.subplot(122)\n",
    "plt.imshow(gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam2 = np.transpose(gam, (2, 0, 1))\n",
    "gam2 = np.expand_dims(gam2, axis=0)\n",
    "print(gam2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = mx.io.NDArrayIter(data = gam2, batch_size=1, shuffle=False)\n",
    "batch = next(test_iter)\n",
    "\n",
    "data = batch.data[0]\n",
    "print(data.shape)\n",
    "#response = sagemaker_predictor.predict(data.asnumpy().tolist())\n",
    "response = sagemaker_predictor.predict(data.asnumpy())\n",
    "output = np.array(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example masks generated from ground truth polygons\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(gam)\n",
    "plt.subplot(122)\n",
    "plt.imshow(post_process_mask(output[0], 0.9), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
