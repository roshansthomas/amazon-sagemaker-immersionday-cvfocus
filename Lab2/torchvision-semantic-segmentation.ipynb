{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1968c8",
   "metadata": {},
   "source": [
    "# Train and deploy a Semantic Segmentation model using pytorch\n",
    "\n",
    "In this lab, you will learn how to train a semantic segmentation model with a model from the [torchvision subpackage](https://pytorch.org/vision/stable/models.html#semantic-segmentation). We will be using the [DeepLabV3 ResNet50 model](https://arxiv.org/abs/1706.05587) and training it on the [SpaceNet dataset](https://spacenet.ai/spacenet-buildings-dataset-v2/).\n",
    "\n",
    "We will be exploring concepts on how to implement a custom model with the PyTorch framework using script mode and learning how to leverage local mode for debugging and testing. \n",
    "\n",
    "Reference:\n",
    "- Examples of Amazon SageMaker Local mode: https://github.com/aws-samples/amazon-sagemaker-local-mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4546b68",
   "metadata": {},
   "source": [
    "## Install libraries and dependencies\n",
    "\n",
    "The following cells will install the required libraries and dependencies on our kernel to support build of this model. One particular library is the solaris tool which will allow us to convert geojson into polygon and back. As the SpaceNet dataset masks are defined in geojson, we will need this utility to convert this file. Optionally, if your downsteam system are expecting a geojson output, we can leverage the solaris library to achieve this.\n",
    "\n",
    "Required libraries. To run this notebook, you will need to install the following dependencies:\n",
    "- rtree\n",
    "- gdal 3.0.3\n",
    "- geopandas\n",
    "- solaris\n",
    "\n",
    "If you are not using the lab2.yml cloudformation template, uncomment the next 3 cells and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f62f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install -c conda-forge rtree gdal=3.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8baf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install solaris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63702440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os, time, json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage import io\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/LAB-pytorch-semantic-segmentation'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import solaris as sol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697eed6",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "We will be downloading the spacenet dataset and uncompressing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://spacenet-dataset/spacenet/SN2_buildings/tarballs/SN2_buildings_train_AOI_3_Paris.tar.gz ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf ./dataset/SN2_buildings_train_AOI_3_Paris.tar.gz -C./dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb80dd9",
   "metadata": {},
   "source": [
    "### Plotting preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ce12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4065fd",
   "metadata": {},
   "source": [
    "## Review data\n",
    "\n",
    "Let's review the dataset we've just downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dataset/AOI_3_Paris_Train/'\n",
    "img_dir = os.path.join(data_dir, 'RGB-PanSharpen')\n",
    "bldg_dir = os.path.join(data_dir, 'geojson', 'buildings')\n",
    "\n",
    "# Prefix of all filename - naming convention\n",
    "midfix = 'AOI_3_Paris_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ee31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_bands(img, lower_pct = 1, upper_pct = 99):\n",
    "    \"\"\"\n",
    "    Rescale the bands of a multichannel image for display\n",
    "    \"\"\"\n",
    "    # Loop through the image bands, rescaling each one\n",
    "    img_scaled = np.zeros(img.shape, np.uint8)\n",
    "    \n",
    "    for i in range(img.shape[2]):\n",
    "        \n",
    "        band = img[:, :, i]\n",
    "        \n",
    "        # Pick out the lower and upper percentiles\n",
    "        lower, upper = np.percentile(band, [lower_pct, upper_pct])\n",
    "        \n",
    "        # Normalize the band\n",
    "        band = (band - lower) / (upper - lower) * 255\n",
    "        \n",
    "        # Clip the high and low values, and cast to uint8\n",
    "        img_scaled[:, :, i] = np.clip(band, 0, 255).astype(np.uint8)\n",
    "        \n",
    "    return img_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b9faa",
   "metadata": {},
   "source": [
    "#### What's happening here?\n",
    "The following cell is loading a sample image and mask from the SpaceNet dataset. We are leveraging the `sol.vector.mask.footprint_mask()` function to convert the geojson file into an array format. We are then using the plotting library to preview our image and mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a data sample\n",
    "sample = 'img100' # chip ID, img? format\n",
    "\n",
    "# Read in 4-channel image from GeoTIFF.\n",
    "img_file = 'RGB-PanSharpen_' + midfix + sample + '.tif'\n",
    "img_path = os.path.join(img_dir, img_file)\n",
    "img = skimage.io.imread(img_path)\n",
    "rgb = img\n",
    "\n",
    "# Read in GeoJSON file and convert polygons to footprint mask.\n",
    "bldg_file = 'buildings_'+ midfix + sample + '.geojson'\n",
    "bldg_path = os.path.join(bldg_dir, bldg_file)\n",
    "mask = sol.vector.mask.footprint_mask(bldg_path, reference_im=img_path)\n",
    "\n",
    "# Display satellite image and building footprint mask.\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(scale_bands(rgb))\n",
    "ax[0].set_title('Satellite image')\n",
    "ax[1].imshow(mask, cmap='Blues')\n",
    "ax[1].set_title('Building footprint masks')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4b677",
   "metadata": {},
   "source": [
    "In the image above, you can see the original satellite image and the building mask generated from the accompanying geojson file. We used the solaris library to conver the geojson into an image so that we can visualise it.\n",
    "\n",
    "**Note** For the satellite image, we created a function `scale_bands()` to process the image as the original image from dataset is an 11-bit image stored in 16 bit integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4adefe",
   "metadata": {},
   "source": [
    "## Generate training data\n",
    "\n",
    "Now that we have some clarity on our dataset, let's process the rest of the images and masks using the `scale_bands()` for the images and `sol.vector.mask.footprint_mask()` function for the mask. We will also split out dataset into training and test and upload it to a designated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir) \n",
    "    \n",
    "training_dir = os.path.join(data_dir, 'train/')\n",
    "test_dir = os.path.join(data_dir, 'test/')\n",
    "\n",
    "if not os.path.exists(training_dir):\n",
    "    os.mkdir(training_dir) \n",
    "    \n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir) \n",
    "\n",
    "training_img_dir = os.path.join(training_dir, 'img/')\n",
    "\n",
    "if not os.path.exists(training_img_dir):\n",
    "    os.mkdir(training_img_dir)\n",
    "    \n",
    "training_mask_dir = os.path.join(training_dir, 'mask/')\n",
    "\n",
    "if not os.path.exists(training_mask_dir):\n",
    "    os.mkdir(training_mask_dir) \n",
    "    \n",
    "    \n",
    "test_img_dir = os.path.join(test_dir, 'img/')\n",
    "\n",
    "if not os.path.exists(test_img_dir):\n",
    "    os.mkdir(test_img_dir)\n",
    "    \n",
    "test_mask_dir = os.path.join(test_dir, 'mask/')\n",
    "\n",
    "if not os.path.exists(test_mask_dir):\n",
    "    os.mkdir(test_mask_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7582f",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of images\n",
    "ListImages=os.listdir(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train = ListImages[0:int(0.7 * len(ListImages))]\n",
    "test = ListImages[int(0.7 * len(ListImages)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5220a",
   "metadata": {},
   "source": [
    "### Process dataset\n",
    "\n",
    "Here we will process our images and masks and save it as a tif file.\n",
    "\n",
    "**Note** Typically this will be done using a [Amazon SageMaker Processing job](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_and_mask(image_list, image_dir, geojson_dir, output_dir):\n",
    "    for img_file in image_list:\n",
    "        print(img_file)\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        img = skimage.io.imread(img_path)\n",
    "        img = scale_bands(img)\n",
    "        \n",
    "        prefix = 'RGB-PanSharpen_' + midfix\n",
    "        if img_file.startswith(prefix):\n",
    "            file_suffix = os.path.splitext(img_file[len(prefix):])[0]\n",
    "            \n",
    "            if os.path.exists(os.path.join(geojson_dir, bldg_file)):\n",
    "                # Create training mask\n",
    "                create_masks(geojson_dir, file_suffix, img_path, output_dir)         \n",
    "        \n",
    "                # Save paired image\n",
    "                output_image_filename = file_suffix + '.tif'\n",
    "                skimage.io.imsave(os.path.join(output_dir,'img', output_image_filename), img, check_contrast=False)\n",
    "\n",
    "def create_masks(geojson_dir, file_suffix, img_path, output_dir):\n",
    "    bldg_file = 'buildings_'+ midfix + file_suffix + '.geojson'\n",
    "    bldg_path = os.path.join(geojson_dir, bldg_file)\n",
    "    output_mask_filename = file_suffix + '.tif'\n",
    "    # Create mask and save\n",
    "    _ = sol.vector.mask.footprint_mask(bldg_path, out_file=os.path.join(output_dir, 'mask/', output_mask_filename), reference_im=img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process our training dataset\n",
    "process_images_and_mask(train, img_dir, bldg_dir, training_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75046c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process our test dataset\n",
    "process_images_and_mask(test, img_dir, bldg_dir, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4b43e",
   "metadata": {},
   "source": [
    "### Upload dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0acae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065e18b",
   "metadata": {},
   "source": [
    "## Setup SageMaker Experiments\n",
    "\n",
    "With [Amazon SageMaker Experiments](https://aws.amazon.com/blogs/aws/amazon-sagemaker-experiments-organize-track-and-compare-your-machine-learning-trainings/), we can track multiple iterations of our training job. With Amazon SageMaker Experiments, you can track the hyperparameters, datasets and algorithms used for each trial and easily compare them. \n",
    "\n",
    "In this section we will setup create an experiment and in the later section create a trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b52cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import strftime\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295654b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "seg_experiment = Experiment.create(\n",
    "    experiment_name=\"spacenet-semantic-segmentation-{}\".format(create_date), \n",
    "    description=\"Semantic Segmentation for the spacenet aerial images\",\n",
    "    tags = [{'Key': 'Environment', 'Value': 'demo1'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1868d",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this lab, we will be training a model using the [bring your own model with script mode](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-script-mode/sagemaker-script-mode.html). To achieve this, we will be using the [PyTorch with SageMaker python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "By using the PyTorch sdk, we will be using a pre-build PyTorch container as our base platform to run our training script. We will provide the container with a custom training script, `script/train.py`. Our custom training script includes a dataloader to load our training and test dataset, our model definition which loads the DeepLabV3 ResNet50 from the torchvision library and supporting function to initialise the neural net and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78035f77",
   "metadata": {},
   "source": [
    "### Test training script in local mode\n",
    "\n",
    "To facilitate debugging your training script, you can train your model using local mode. This can be achieved by setting the `instance_type` variable to `local`. The following code will execute a training job for one epoch.\n",
    "\n",
    "#### What's happening here?\n",
    "The following code is launching a local training job (`instance_type='local'`) with a pytorch framework version of 1.8.0 (`framework_version='1.8.0'`). The training job will load our training script **train.py** (`entry_point=train.py`) from our source directory **script** (`source_dir=script`). Within the `hyperparameters` variable, we set the training job to run for 1 epoch, a batch size of 16 and a learning rate of 0.0005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f948b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir=\"script\",\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    framework_version='1.8.0',\n",
    "                    instance_count=1,\n",
    "                    instance_type='local',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 1,\n",
    "                        'batch-size': 16,\n",
    "                        'lr': 0.0005,\n",
    "                        'log-interval': 10\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40689332",
   "metadata": {},
   "source": [
    "### Execute a SageMaker Job\n",
    "\n",
    "Once you're satisfied with your training script, the next process is to scale your model training by leveraging an Amazon SageMaker training job. To do so we will set the `instance_type` to a specific [Amazon SageMaker Instance types](https://aws.amazon.com/sagemaker/pricing/). In this example, we will be using a `ml.g4dn.2xlarge` instance that has a gpu.\n",
    "\n",
    "When the `fit()` function is called, an Amazon SageMaker training job will be initialised. If you have access to the console, you will be able visualise your training job execution [https://ap-southeast-2.console.aws.amazon.com/sagemaker/home?region=ap-southeast-2#/jobs](https://ap-southeast-2.console.aws.amazon.com/sagemaker/home?region=ap-southeast-2#/jobs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_trial = Trial.create(trial_name = \"spacenet-semantic-segmentation-{}-{}\".format(create_date, int(time.time())),\n",
    "                          experiment_name = seg_experiment.experiment_name,\n",
    "                          tags = [{'Key': 'Environment', 'Value': 'demo1'}])\n",
    "\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir=\"script\",\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    framework_version='1.8.0',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.g4dn.4xlarge',\n",
    "                    volume_size=50,\n",
    "                    hyperparameters={\n",
    "                        'epochs': 5,\n",
    "                        'lr': 0.005,\n",
    "                        'batch-size': 16,\n",
    "                        'log-interval': 10\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'training': inputs}, experiment_config = {\n",
    "        # \"ExperimentName\"\n",
    "        \"TrialName\" : seg_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\" : \"TrainingJob\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa39bed",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e3b6d",
   "metadata": {},
   "source": [
    "## Create a real-time endpoint\n",
    "As we are using a custom model and will be sending images as an input, we will need to override the default mechanism of how Amazon SageMaker inference container loads our model, process the input request and output response. This is achieved by overriding the following respective functions implemented in the `script/inference.py` file:\n",
    "\n",
    "- model_fn: Override the model loading function to load the deeplabv3_resnet50 model with weights from our training job.\n",
    "- input_fn: Override the input function convert the incoming image payload into a tensor suitable for prediction/\n",
    "- output_fn: Override the output function to convert the output prediction into a numpy array.\n",
    "\n",
    "More information on how to override the inference functions here:\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html\n",
    "\n",
    "#### What is happening here?\n",
    "We are using the `PyTorchModel()` class to define our real-time inference configuration. The trained model is parse through the `model_data` parameter and the source of the inference script is specified through the `source_dir` for the directory and `entry_point` for the script.\n",
    "\n",
    "The real-time inference endpoint is then deployed using the `deploy()` function where we specify the initial number of instances (`initial_instance_count`) and desired  instance type (`instance_type`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_serving_model = PyTorchModel(\n",
    "    model_data=estimator.model_data,\n",
    "    role=role,\n",
    "    framework_version='1.8.0',\n",
    "    py_version='py3',\n",
    "    entry_point='inference.py',\n",
    "    source_dir=\"script\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d150323",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosted_predictor = pytorch_serving_model.deploy(initial_instance_count=1,\n",
    "        instance_type='ml.m4.4xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosted_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13a4b8",
   "metadata": {},
   "source": [
    "### Run some predictions\n",
    "\n",
    "Now that our real-time endpoint is up, let us run some predictions. For this lab, as we only train a model is a few epoch, we won't be expecting an accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f4e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a test image\n",
    "# img_path = './data/test/img/img100.tif'\n",
    "# mask_path = './data/test/mask/img100.tif'\n",
    "\n",
    "# Using a train image\n",
    "img_path = './data/train/img/img1643.tif'\n",
    "mask_path = './data/train/mask/img1643.tif'\n",
    "\n",
    "with open(img_path, \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "sm_runtime = boto3.Session().client(\"sagemaker-runtime\")\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=hosted_predictor.endpoint_name, ContentType=\"application/x-image\", Body=payload\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2cdc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the output response\n",
    "result = json.loads(response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(result).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(img_path, \"rb\") as image_file, open(mask_path, \"rb\") as mask_file:\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Satellite image')\n",
    "    ax[1].imshow(np.array(result)[0] > 0.5, cmap='Blues')\n",
    "    ax[1].set_title('Building masks prediction')\n",
    "    ax[2].imshow(mask, cmap='Blues')\n",
    "    ax[2].set_title('Building ground truth')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97db21",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "As the real-time inference endpoint is running 24/7, it is often best practice to always delete the endpoint once we are done with testing. The `delete_endpoint()` function will delete our running endpoint and its associated configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosted_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30f468",
   "metadata": {},
   "source": [
    "## Test local inference\n",
    "\n",
    "As we are overriding the inference functions with a custom script, it may be useful to run our inference locally to test and debug inference script prior to deploying it in the cloud. The folowing section demonstrates how you can achieve this.\n",
    "\n",
    "#### What's happening here?\n",
    "Similar to the above function, we are defining an instance of the PyTorchModel. However we are setting the session to a local session (`sagemaker_session=LocalSession()`).\n",
    "\n",
    "In the `deploy()` function, we set the `instance_type` to `local`. When we run the deploy function, the sdk will launch a docker container within our notebook environment using a pre-built pytorch container whilst uploading our inference script and loading it with our model weights from s3. If you're interested in learning more, you can launch a terminal in a new tab and run `docker ps` and you will see an inference container running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "pytorch_local_serving_model = PyTorchModel(\n",
    "    model_data=estimator.model_data,\n",
    "    role=role,\n",
    "    framework_version='1.8.0',\n",
    "    py_version='py3',\n",
    "    sagemaker_session=LocalSession(),\n",
    "    entry_point='inference.py',\n",
    "    source_dir=\"script\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_predictor = pytorch_local_serving_model.deploy(initial_instance_count=1,\n",
    "        instance_type='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c08c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(img_path, \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "    \n",
    "response = local_predictor.predict(payload, initial_args={'ContentType': 'application/x-image'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
